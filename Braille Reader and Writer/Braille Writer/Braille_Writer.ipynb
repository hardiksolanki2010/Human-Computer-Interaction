{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Instaling necessary libraties"
      ],
      "metadata": {
        "id": "wGamlf_3X7tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBogCpzwYhPF",
        "outputId": "9ce892ad-36fb-478b-b235-0d7de0891a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 Speech to Text Conversion"
      ],
      "metadata": {
        "id": "jGASSk6XYDAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe('/content/speech1.mp4')\n",
        "print(result[\"text\"])\n",
        "\n",
        "def speech_to_text(audio_file, output_text):\n",
        "    # Load the Whisper model\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe('/content/speech1.mp4')\n",
        "\n",
        "    # Save the transcription to a text file\n",
        "    with open(output_text, 'w') as file:\n",
        "        file.write(result['text'])\n",
        "\n",
        "    print(f\"Transcription saved to {output_text}\")\n",
        "\n",
        "# Usage example\n",
        "audio_file = '/content/speech1.mp4 '   # replace with the path to your audio file\n",
        "output_text = 'transcription.txt'\n",
        "speech_to_text(audio_file, output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4ddyc0FZWPj",
        "outputId": "2a1660dc-d6c0-470c-f665-551d33ca0741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " HCI is the study of how humans interact with computers and how to design computer systems that are easy, quick and productive for humans to use. Human, the user of a digital hardware-based, device or software application. Computer, the digital, physical, device, hardware or artifact that provides some service to the user, typically via a computer program interaction, the communication, information exchange, between the human, user and the digital device-slash software application, computer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription saved to transcription.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 Text to Braille Conversion"
      ],
      "metadata": {
        "id": "DXqtjQwPYHPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Braille character mappings (simplified for demonstration)\n",
        "braille_alphabet = {\n",
        "    'a': '⠁', 'b': '⠃', 'c': '⠉', 'd': '⠙', 'e': '⠑', 'f': '⠋', 'g': '⠛', 'h': '⠓',\n",
        "    'i': '⠊', 'j': '⠚', 'k': '⠅', 'l': '⠇', 'm': '⠍', 'n': '⠝', 'o': '⠕', 'p': '⠏',\n",
        "    'q': '⠟', 'r': '⠗', 's': '⠎', 't': '⠞', 'u': '⠥', 'v': '⠧', 'w': '⠺', 'x': '⠭',\n",
        "    'y': '⠽', 'z': '⠵', ' ': ' ', ',': '⠂', '.': '⠲', '?': '⠦', '!': '⠖', '-': '⠤',\n",
        "    '\\n': '\\n'\n",
        "}\n",
        "\n",
        "def text_to_braille(input_text, output_braille):\n",
        "    # Read the text from the input file\n",
        "    with open('/content/transcription.txt', 'r') as file:\n",
        "        text = file.read().lower()\n",
        "\n",
        "    # Convert each character to Braille\n",
        "    braille_text = ''.join(braille_alphabet.get(char, '') for char in text)\n",
        "\n",
        "    # Write the Braille text to the output file\n",
        "    with open(output_braille, 'w') as file:\n",
        "        file.write(braille_text)\n",
        "\n",
        "    print(f\"Braille conversion saved to {output_braille}\")\n",
        "\n",
        "# Usage example\n",
        "input_text = 'transcription.txt'\n",
        "output_braille = 'braille_output.txt'\n",
        "text_to_braille(input_text, output_braille)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bm_8-GCac5-",
        "outputId": "c3b99cb2-4f34-4bf1-c16e-2fee466b9370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Braille conversion saved to braille_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Braille Writer Final Program\n",
        "#### Merging all the above logic"
      ],
      "metadata": {
        "id": "GaEXVDz7YLrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Braille character mappings (simplified)\n",
        "braille_alphabet = {\n",
        "    'a': '⠁', 'b': '⠃', 'c': '⠉', 'd': '⠙', 'e': '⠑', 'f': '⠋', 'g': '⠛', 'h': '⠓',\n",
        "    'i': '⠊', 'j': '⠚', 'k': '⠅', 'l': '⠇', 'm': '⠍', 'n': '⠝', 'o': '⠕', 'p': '⠏',\n",
        "    'q': '⠟', 'r': '⠗', 's': '⠎', 't': '⠞', 'u': '⠥', 'v': '⠧', 'w': '⠺', 'x': '⠭',\n",
        "    'y': '⠽', 'z': '⠵', ' ': ' ', ',': '⠂', '.': '⠲', '?': '⠦', '!': '⠖', '-': '⠤',\n",
        "    '\\n': '\\n'\n",
        "}\n",
        "\n",
        "def speech_to_braille(audio_file, output_braille):\n",
        "    # Load the Whisper model for transcription\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Step 1: Transcribe the audio file to text\n",
        "    print(\"Transcribing audio to text...\")\n",
        "    result = model.transcribe('/content/speech1.mp4')\n",
        "    transcription = result['text']\n",
        "    print(\"Transcription complete.\")\n",
        "\n",
        "    # Step 2: Convert transcribed text to Braille\n",
        "    print(\"Converting text to Braille...\")\n",
        "    braille_text = ''.join(braille_alphabet.get(char, '') for char in transcription.lower())\n",
        "\n",
        "    # Write the Braille text to an output file\n",
        "    with open('/content/braille_output.txt', 'w') as file:\n",
        "        file.write(braille_text)\n",
        "\n",
        "    print(f\"Braille text saved to {output_braille}\")\n",
        "\n",
        "# Usage example\n",
        "audio_file = '/content/speech1.mp4'     # replace with your audio file path\n",
        "output_braille = '/content/braille_output.txt'\n",
        "speech_to_braille(audio_file, output_braille)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpMFB0zPa1c8",
        "outputId": "ae99eb23-e25c-4daa-b41f-dc6c1b1da10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing audio to text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription complete.\n",
            "Converting text to Braille...\n",
            "Braille text saved to /content/braille_output.txt\n"
          ]
        }
      ]
    }
  ]
}